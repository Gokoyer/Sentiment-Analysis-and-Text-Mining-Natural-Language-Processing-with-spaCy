{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b9622d-1a8e-4780-bfc7-f93b801e6819",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis at position 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(parsed)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 7) Execute the Cleaning\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m parsed_df \u001b[38;5;241m=\u001b[39m \u001b[43mparse_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 8) Merge the cleaned comments DataFrame with our movie list DataFrame\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#    to get the movie titles for each YouTube ID.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m#    Ensure movie_list has the columns: 'youtubeId' and 'title'\u001b[39;00m\n\u001b[0;32m    100\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(parsed_df, movie_list, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutubeId\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 79\u001b[0m, in \u001b[0;36mparse_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Apply slang replacement to each valid comment line.\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_slang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Keep the cleaned line\u001b[39;00m\n\u001b[0;32m     82\u001b[0m comments\u001b[38;5;241m.\u001b[39mappend(line)\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mreplace_slang\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_slang\u001b[39m(text):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m slang, replacement \u001b[38;5;129;01min\u001b[39;00m slang_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# dynamic, safe pattern for each slang word\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mslang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIGNORECASE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_compiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    742\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 743\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:987\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munbalanced parenthesis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgrouprefpos:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgroups:\n",
      "\u001b[1;31merror\u001b[0m: unbalanced parenthesis at position 5"
     ]
    }
   ],
   "source": [
    "# 1) Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 2) Define File Paths\n",
    "# It's good practice to store file names in variables for easy modification.\n",
    "file_path_text = 'NoisyText.txt'            # raw noisy text file\n",
    "file_path_csv  = 'vdoLinks.csv'           # CSV containing at least: youtubeId, title\n",
    "output_path    = 'Cleaned_Comments_Output.txt'\n",
    "\n",
    "# 3) Load Data\n",
    "# We load the movie links from the CSV file into a pandas DataFrame.\n",
    "movie_list = pd.read_csv(file_path_csv)\n",
    "\n",
    "# We load the raw, noisy text file. The 'latin-1' encoding helps avoid codec errors\n",
    "# if the text has odd characters.\n",
    "with open(file_path_text, 'r', encoding='latin-1') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# 4) Calculate Initial Noise Metrics\n",
    "# We calculate the total number of letters in the original file.\n",
    "# The regex [A-Za-z] matches all uppercase and lowercase letters.\n",
    "letters_before = len(re.findall(r'[A-Za-z]', raw_data))\n",
    "\n",
    "# 5) Define Slang and Replacement Logic\n",
    "# The lab requires replacing slang words with \"(**)\" A dictionary.\n",
    "# is a perfect way to store these word replacement pairs\n",
    "slang_dict = {\n",
    "    \"lol\":  \"(**)\", \"omg\":  \"(**)\", \"wtf\": \"(**)\",\n",
    "    \"idk\": \"(**)\", \"btw\": \"(**)\", \"lmao\": \"(**)\"\n",
    "}\n",
    "\n",
    "#This function uses a regular expression to perform case-insensitive\n",
    "#replacement of slang words. The '\\b' ensures we only match whole words\n",
    "# and not parts of other words (e.g., 'lol' in lollopop').\n",
    "def replace_slang(text):\n",
    "    for slang, replacement in slang_dict.items():\n",
    "        # dynamic, safe pattern for each slang word\n",
    "        text = re.sub(rf\"\\b{slang})\\b\", replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# 6) Core Parsing and Cleaning Function\n",
    "def parse_data(data):\n",
    "    # IMPORTANT: Replace this delimiter if your file uses a different one.\n",
    "    sections = data.split('NewMovieDrPQRd')\n",
    "    parsed = []\n",
    "\n",
    "    # Loop through each section to extract the movie ID and comments.\n",
    "    for section in sections:\n",
    "        # Split each section into individual lines.\n",
    "        lines = section.strip().split('\\n')\n",
    "\n",
    "        # Skip empty sections\n",
    "        if not lines or len(lines[0].strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        # The first line of each valid section is the YouTube ID.\n",
    "        movie_id = lines[0].strip()\n",
    "        comments = []\n",
    "\n",
    "        # Iterate through the rest of the lines to extract comments.\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Skip lines that are either empty or a repeat of the movie ID.\n",
    "            if not line or line == movie_id:\n",
    "                continue\n",
    "\n",
    "            # Skip lines that are identified as noisy text (URLs, system messages, etc.)\n",
    "            if line.startswith('<') or line.startswith(\"'\"):\n",
    "                continue\n",
    "            if any(skip in line.lower() for skip in [\n",
    "                'http error', 'charmap', 'pt1m', '128238',\n",
    "                'video has disabled comments'\n",
    "            ]):\n",
    "                continue\n",
    "\n",
    "            # Apply slang replacement to each valid comment line.\n",
    "            line = replace_slang(line)\n",
    "\n",
    "            # Keep the cleaned line\n",
    "            comments.append(line)\n",
    "\n",
    "        # If no valid comments were found for a movie, add a specific message.\n",
    "        if not comments:\n",
    "            comments = ['No comments were found']\n",
    "\n",
    "        # Create a dictionary for each movie and add it to our parsed list.\n",
    "        parsed.append({'youtubeId': movie_id, 'comments': comments})\n",
    "\n",
    "    # Convert list of dicts into a DataFrame for easy merging and outputting.\n",
    "    return pd.DataFrame(parsed)\n",
    "\n",
    "# 7) Execute the Cleaning\n",
    "parsed_df = parse_data(raw_data)\n",
    "\n",
    "# 8) Merge the cleaned comments DataFrame with our movie list DataFrame\n",
    "#    to get the movie titles for each YouTube ID.\n",
    "#    Ensure movie_list has the columns: 'youtubeId' and 'title'\n",
    "merged_df = pd.merge(parsed_df, movie_list, on='youtubeId', how='left')\n",
    "\n",
    "# 9) Write the Output\n",
    "# We write the cleaned output, formatted as requested.\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # Movie title (falls back to the ID if title is missing)\n",
    "        #movie_title = row['title'] if ('title' in row and pd.notna(row['title'])) else row['youtubeId']\n",
    "        f.write(f\"Movie Name: {row['title']}\\n\")\n",
    "        if row['Comments'] == ['No comments were found']:\n",
    "            f.write(\"No comments were found\\n\")\n",
    "        else:\n",
    "            f.write(\"The comments are:\\n\")\n",
    "            for comment in row['comments']:\n",
    "                f.write(comment + '\\n')\n",
    "            f.write('\\n')  # blank line between movie sections\n",
    "\n",
    "# 10) Calculate and Display Cleaning Efficiency\n",
    "# After writing the cleaned text, we read it back to count the letters.\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    cleaned_text = f.read()\n",
    "\n",
    "# Count letters in the cleaned text.\n",
    "letters_after = len(re.findall(r'[A-Za-z]', cleaned_text))\n",
    "\n",
    "# We calculate the ratio of letters after cleaning to letters before cleaning.\n",
    "# A ratio less than one indicates that noisy text was successfully removed.\n",
    "ratio = (letters_after / letters_before)\n",
    "print('Letters Before:', letters_before)\n",
    "print('Letters After :', letters_after)\n",
    "print('Cleaning Ratio:', round(ratio, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e2654-7201-4c82-ad91-d67b4c1a2728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
